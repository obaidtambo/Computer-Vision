{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEpgJaTdtHUx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmGhk1GdIWoc"
      },
      "source": [
        "##Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTsv84wXUi8e"
      },
      "source": [
        "In this approach, we have applied a function ‘hog_detect’ for evaluating the hog descriptions of each image of the handwritten digits datasets and saved the output of each descriptions (which is an array) into an array which works as the feature set for the SVM model. We have applied SVC and applied polynomial kernel as the datasets are divided into multiple labels rather than being binary labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWW5yQr6Ua5O"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define the larger model\n",
        "def larger_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu'))\n",
        "\tmodel.add(Dense(50, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# build the model\n",
        "model = larger_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ6HyZNxUa2G"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0]) #Test loss: 0.0296396646054\n",
        "print('Test accuracy:', score[1]) #Test accuracy: 0.990"
      ]
    }
  ]
}