{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeVRp7PerJBe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIFT"
      ],
      "metadata": {
        "id": "IEC750a_rhWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqZVM3VlIi7u"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "image_shape=size(image) \n",
        "images= Array [ ] \n",
        "for size in 1:5 \n",
        "resized_image=image.reshape(image_shape//size) \n",
        "insert resized_image into images \n",
        "end\n",
        "blurred_images=[ ] \n",
        "sigmas=Array[] # predecided values of sigma for example from 0.1,1.5,3.0,4.5,6.0 \n",
        "for i in 1:5 \n",
        "{\n",
        "  scale=Array [] \n",
        "for sigma in sigmas {\n",
        "insert GauassianBlur(images[ i ], sigma) into scale \n",
        "} \n",
        "insert scale into blurred_images\n",
        "}\n",
        "DOG=Array [] for i in blurred_images \n",
        "\n",
        "scale=Array [] \n",
        "for index in 1:length(i)-1 {\n",
        "insert (i[index]-i[index+1]) into Scale \n",
        "} \n",
        "insert scale into DOG \n",
        "keypoint= Array [] \n",
        "for each scale_image in DOG \n",
        "for each image in scale_image \n",
        "for each pixel in image \n",
        "if image[pixel]>image[8-neighbourhood] and image[pixel] > other images[pixel] in sam \n",
        "insert pixel into keypoint\n",
        "remove keypoints if near an edge or if keypoint is a low contrast pixel orientation=[] \n",
        "for keypoint in keypoints: \n",
        "a neighbourhood of pixels are selected around the keypoint gradient_magnitude,gradient_direction are found in that region all points above 80% of max maginitude are selected and considered to calculate orientation, add such points on orientation subtract localised orientation to make it rotation independent.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7c5_TOYlZs"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "SIFT is invariant to scale and orientation, making it possible to extract the same features from images taken from different viewpoints of an object, SIFT extracts those features where are scale and rotation invariant and these keypoints can be used as features for object detection , etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Harris Coner Detector for Feature matching"
      ],
      "metadata": {
        "id": "pcdo6SDs0spu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eZZ-99JIwuc"
      },
      "outputs": [],
      "source": [
        "img51 = cv2.imread('/content/eiffel2.jpg')\n",
        "img52 = cv2.imread('/content/eiffel 3.jpg')\n",
        "def doHarris(img):\n",
        "  grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  grayscale = np.float32(grayscale)\n",
        "  dst = cv2.cornerHarris(grayscale, 2, 3, 0.04)\n",
        "  dst = cv2.dilate(dst, None)\n",
        "  img[dst>0.01*dst.max()] = [0,0,255]\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrWhqXRGIwr8"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(doHarris(img51))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVHo4GLvIwpE"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(doHarris(img52))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csIDCMj1uJ3c"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "#images from diff view of same monument and applying Harris corner to it.\n",
        "img1 = cv2.imread(\"/content/eiffel2.jpg\")\n",
        "Img1= cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(Img1)\n",
        "plt.show()\n",
        "img2 = cv2.imread(\"/content/eiffel 3.jpg\")\n",
        "Img2= cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(Img2)\n",
        "plt.show()\n",
        "\n",
        "# grayscale color space\n",
        "Img1= cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "Img2= cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# setting to 32-bit floating point\n",
        "Img1 = np.float32(Img1)\n",
        "Img2 = np.float32(Img2)\n",
        "\n",
        "# apply the cv2.cornerHarris method\n",
        "# to detect the corners with appropriate\n",
        "# values as input parameters\n",
        "dest1 = cv2.cornerHarris(Img1, 2, 5, 0.07)\n",
        "dest2 = cv2.cornerHarris(Img2, 2, 5, 0.07)\n",
        "\n",
        "# Results are marked through the dilated corners\n",
        "dest1 = cv2.dilate(dest1, None)\n",
        "dest2 = cv2.dilate(dest2, None)\n",
        "\n",
        "\n",
        "# Reverting back to the original image,\n",
        "# with optimal threshold value\n",
        "img1[dest1 > 0.01 * dest1.max()]=[0, 0, 255]\n",
        "\n",
        "\n",
        "\n",
        "cv2_imshow(img1)\n",
        "\n",
        "# Reverting back to the original image,\n",
        "# with optimal threshold value\n",
        "\n",
        "img2[dest2 > 0.01 * dest2.max()]=[0, 0, 255]\n",
        "\n",
        "cv2_imshow(img2)\n",
        "\n",
        "plt.imshow(img1)\n",
        "plt.show()\n",
        "plt.imshow(img2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y7Uv2ydIwmd"
      },
      "outputs": [],
      "source": [
        "query_img_bw = cv2.cvtColor(img51, cv2.COLOR_BGR2GRAY)\n",
        "train_img_bw = cv2.cvtColor(img52, cv2.COLOR_BGR2GRAY)\n",
        "orb = cv2.ORB_create()\n",
        "queryKeypoints, queryDescriptors = orb.detectAndCompute(query_img_bw, None)\n",
        "trainKeypoints, trainDescriptors = orb.detectAndCompute(train_img_bw, None)\n",
        "matcher = cv2.BFMatcher()\n",
        "matches = matcher.match(queryDescriptors, trainDescriptors)\n",
        "final_img = cv2.drawMatches(img51, queryKeypoints, img52, trainKeypoints, matches[:20], None)\n",
        "final_img = cv2.resize(final_img, (1000, 650))\n",
        "cv2_imshow(final_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULnBWH2NdDum"
      },
      "source": [
        "Harris corner detection algorithm identifies the internal corners of an image. The corners of an image are basically identified as the regions in which there are variations in large intensity of the gradient in all possible dimensions and directions.\n",
        "Harris corner detection works as:\n",
        "1. Take the grayscale image of the original one\n",
        "2. Apply Sobel operator to find the x and y gradient values for every pixel in the grayscale image\n",
        "3. For each pixel p in the grayscale image, consider a 3x3 window around it and compute the corner strength function.  Call this its Harris value.\n",
        "4. Find all pixels that exceed a certain threshold and are the local maxima within a certain window.\n",
        "5. For each pixel that meets the criteria in 4, compute a feature descriptor\n",
        "\n",
        "\n",
        "ORB use both the FAST keypoint detector and BRIEF descriptor with some added features to improve the performance. FAST is Features from Accelerated Segment Test used to detect features from the provided image.\n",
        "ORB uses BRIEF descriptor but as the BRIEF performs poorly with rotation. ORB is an efficient alternative to SIFT or SURF algorithms used for feature extraction, in computation cost, matching performance and mainly the patents.\n",
        "We have used the following algorithm:\n",
        "1. Take the input image and convert it into grayscale\n",
        "2. Initialize the ORB detector and detect the keypoints in the input image\n",
        "3. Now we have computed the descriptors belonging to both the images\n",
        "4. Then, the keypoints were matched using the Brute Force matcher\n",
        "5. Show the matched images\n",
        "For this question, we have used the image from two different perspectives."
      ]
    }
  ]
}